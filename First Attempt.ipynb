{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/10], Step [0/782], Loss: 0.3865\n",
      "Epoch [1/10], Step [100/782], Loss: 0.0320\n",
      "Epoch [1/10], Step [200/782], Loss: 0.0245\n",
      "Epoch [1/10], Step [300/782], Loss: 0.0133\n",
      "Epoch [1/10], Step [400/782], Loss: 0.0075\n",
      "Epoch [1/10], Step [500/782], Loss: 0.0059\n",
      "Epoch [1/10], Step [600/782], Loss: 0.0051\n",
      "Epoch [1/10], Step [700/782], Loss: 0.0039\n",
      "Epoch [2/10], Step [0/782], Loss: 0.0037\n",
      "Epoch [2/10], Step [100/782], Loss: 0.0033\n",
      "Epoch [2/10], Step [200/782], Loss: 0.0040\n",
      "Epoch [2/10], Step [300/782], Loss: 0.0027\n",
      "Epoch [2/10], Step [400/782], Loss: 0.0032\n",
      "Epoch [2/10], Step [500/782], Loss: 0.0023\n",
      "Epoch [2/10], Step [600/782], Loss: 0.0021\n",
      "Epoch [2/10], Step [700/782], Loss: 0.0019\n",
      "Epoch [3/10], Step [0/782], Loss: 0.0021\n",
      "Epoch [3/10], Step [100/782], Loss: 0.0017\n",
      "Epoch [3/10], Step [200/782], Loss: 0.0017\n",
      "Epoch [3/10], Step [300/782], Loss: 0.0015\n",
      "Epoch [3/10], Step [400/782], Loss: 0.0014\n",
      "Epoch [3/10], Step [500/782], Loss: 0.0012\n",
      "Epoch [3/10], Step [600/782], Loss: 0.0011\n",
      "Epoch [3/10], Step [700/782], Loss: 0.0013\n",
      "Epoch [4/10], Step [0/782], Loss: 0.0011\n",
      "Epoch [4/10], Step [100/782], Loss: 0.0010\n",
      "Epoch [4/10], Step [200/782], Loss: 0.0010\n",
      "Epoch [4/10], Step [300/782], Loss: 0.0009\n",
      "Epoch [4/10], Step [400/782], Loss: 0.0009\n",
      "Epoch [4/10], Step [500/782], Loss: 0.0010\n",
      "Epoch [4/10], Step [600/782], Loss: 0.0009\n",
      "Epoch [4/10], Step [700/782], Loss: 0.0009\n",
      "Epoch [5/10], Step [0/782], Loss: 0.0008\n",
      "Epoch [5/10], Step [100/782], Loss: 0.0009\n",
      "Epoch [5/10], Step [200/782], Loss: 0.0008\n",
      "Epoch [5/10], Step [300/782], Loss: 0.0006\n",
      "Epoch [5/10], Step [400/782], Loss: 0.0007\n",
      "Epoch [5/10], Step [500/782], Loss: 0.0007\n",
      "Epoch [5/10], Step [600/782], Loss: 0.0006\n",
      "Epoch [5/10], Step [700/782], Loss: 0.0007\n",
      "Epoch [6/10], Step [0/782], Loss: 0.0005\n",
      "Epoch [6/10], Step [100/782], Loss: 0.0006\n",
      "Epoch [6/10], Step [200/782], Loss: 0.0006\n",
      "Epoch [6/10], Step [300/782], Loss: 0.0005\n",
      "Epoch [6/10], Step [400/782], Loss: 0.0005\n",
      "Epoch [6/10], Step [500/782], Loss: 0.0006\n",
      "Epoch [6/10], Step [600/782], Loss: 0.0007\n",
      "Epoch [6/10], Step [700/782], Loss: 0.0005\n",
      "Epoch [7/10], Step [0/782], Loss: 0.0005\n",
      "Epoch [7/10], Step [100/782], Loss: 0.0006\n",
      "Epoch [7/10], Step [200/782], Loss: 0.0005\n",
      "Epoch [7/10], Step [300/782], Loss: 0.0005\n",
      "Epoch [7/10], Step [400/782], Loss: 0.0005\n",
      "Epoch [7/10], Step [500/782], Loss: 0.0004\n",
      "Epoch [7/10], Step [600/782], Loss: 0.0005\n",
      "Epoch [7/10], Step [700/782], Loss: 0.0004\n",
      "Epoch [8/10], Step [0/782], Loss: 0.0005\n",
      "Epoch [8/10], Step [100/782], Loss: 0.0005\n",
      "Epoch [8/10], Step [200/782], Loss: 0.0004\n",
      "Epoch [8/10], Step [300/782], Loss: 0.0004\n",
      "Epoch [8/10], Step [400/782], Loss: 0.0004\n",
      "Epoch [8/10], Step [500/782], Loss: 0.0004\n",
      "Epoch [8/10], Step [600/782], Loss: 0.0004\n",
      "Epoch [8/10], Step [700/782], Loss: 0.0003\n",
      "Epoch [9/10], Step [0/782], Loss: 0.0004\n",
      "Epoch [9/10], Step [100/782], Loss: 0.0004\n",
      "Epoch [9/10], Step [200/782], Loss: 0.0003\n",
      "Epoch [9/10], Step [300/782], Loss: 0.0003\n",
      "Epoch [9/10], Step [400/782], Loss: 0.0005\n",
      "Epoch [9/10], Step [500/782], Loss: 0.0004\n",
      "Epoch [9/10], Step [600/782], Loss: 0.0004\n",
      "Epoch [9/10], Step [700/782], Loss: 0.0003\n",
      "Epoch [10/10], Step [0/782], Loss: 0.0003\n",
      "Epoch [10/10], Step [100/782], Loss: 0.0004\n",
      "Epoch [10/10], Step [200/782], Loss: 0.0003\n",
      "Epoch [10/10], Step [300/782], Loss: 0.0003\n",
      "Epoch [10/10], Step [400/782], Loss: 0.0003\n",
      "Epoch [10/10], Step [500/782], Loss: 0.0003\n",
      "Epoch [10/10], Step [600/782], Loss: 0.0003\n",
      "Epoch [10/10], Step [700/782], Loss: 0.0002\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Define the Encoder Network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "# Define the Decoder Network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.deconv4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.deconv4(x))\n",
    "        x = self.tanh(self.deconv5(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate the networks and move them to the appropriate device\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "# Freeze the encoder parameters\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.0002)\n",
    "\n",
    "# Data loading and transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Using CIFAR-10 dataset for demonstration\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        # Move data to the appropriate device\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Encode and then Decode\n",
    "        with torch.no_grad():\n",
    "            latent_rep = encoder(data)\n",
    "        output = decoder(latent_rep)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output, data)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_in_bytes = param_size + buffer_size\n",
    "    size_in_megabytes = size_in_bytes / (1024 ** 2)\n",
    "    \n",
    "    return size_in_megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ssim(im1, im2):\n",
    "    ssim_value = np.mean([\n",
    "        ssim(im1, im2, data_range=im2.max() - im2.min())\n",
    "    ])\n",
    "    \n",
    "    return ssim_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average SSIM:  0.9763615762057603\n",
      "Size of encoder 0.512451171875 MB\n",
      "Size of decoder 0.5119743347167969 MB\n",
      "Size of latent representation of image: 0.078125 KB\n"
     ]
    }
   ],
   "source": [
    "ssim_list = []\n",
    "for i in data:\n",
    "    latent = encoder(i)\n",
    "    output = decoder(latent)\n",
    "    for j in range(3):\n",
    "        ssim_output = calculate_ssim(i.to('cpu').detach().numpy()[j], output.to('cpu').detach().numpy()[j])\n",
    "        ssim_list.append(ssim_output)\n",
    "\n",
    "encoder_size = get_model_size(encoder)\n",
    "decoder_size = get_model_size(decoder)\n",
    "ssim_list = np.array(ssim_list)\n",
    "print(\"Average SSIM: \", ssim_list.mean())\n",
    "encoder_size = get_model_size(encoder)\n",
    "decoder_size = get_model_size(decoder)\n",
    "print('Size of encoder {} MB'. format(encoder_size))\n",
    "print('Size of decoder {} MB'. format(decoder_size))\n",
    "print('Size of latent representation of image: {} KB'.format(sys.getsizeof(latent)/1024))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
